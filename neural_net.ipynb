{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alphanumeric Image Classifier\n",
        "In this project, we learn how to create a simple Neural Network.\n",
        "\n",
        "The goal is to create a model capable of recognising an alphanumeric character (A-Z, a-z, 0-9).  \n",
        "The input will be a 32x24 pixel image. Any image we use will be downscaled to fit this size.\n",
        "\n",
        "#### What technologies did we use?\n",
        "We are not using Tensorflow or Pytorch.  \n",
        "We are only using simple functions and classes to represent perceptrons etc.\n",
        "\n",
        "Created by: andrewpublic & joshuao1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Q9h3tQ86KrMf"
      },
      "outputs": [],
      "source": [
        "# load data set\n",
        "\n",
        "# create training data sets\n",
        "\n",
        "# feed the training data to my model\n",
        "\n",
        "# evaluation function on training data\n",
        "\n",
        "# score it against global data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Input Preprocessing\n",
        "We have a dataset with 3400~ images.  \n",
        "We first downsize the images using Pillow.\n",
        "\n",
        "(Todo: Add python code related to input preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ptccM9p5hUj3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training Method\n",
        "\n",
        "We are going to use a class to represent perceptrons. With some helper functions below.\n",
        "\n",
        "Our approach will be to initialise all the perceptrons/neurons for our Neural Network.\n",
        "\n",
        "- We'll then apply backpropagation using gradient descent.\n",
        "\n",
        "(We won't use any fancy types of Neural Networks like CNN yet which may be better for image classification models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "fgrqhaZrMHHj"
      },
      "outputs": [],
      "source": [
        "###  ****** We are not using this yet or ever *****\n",
        "\n",
        "### Function to evaluate performance\n",
        "def calc_squared_error(y_hat, y):\n",
        "  return (y_hat-y)**2\n",
        "\n",
        "def dumb_evalutor(y_hat, y):\n",
        "  count = 0\n",
        "  for i in range(y_hat):\n",
        "    if y_hat[i] == y[i]:\n",
        "      count +=1\n",
        "  return count / len(y_hat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjyr1H0uPJix",
        "outputId": "aa4f5a54-b50a-4411-923d-77e0d927ab94"
      },
      "outputs": [],
      "source": [
        "# ### Coinflip model\n",
        "\n",
        "\n",
        "\n",
        "# def character_classifier_model(df_x, y):\n",
        "#     perceptron_array = initialise_perceptron_layers()\n",
        "#     return perceptron_array\n",
        "\n",
        "# ### Helper visualisation functions\n",
        "# def print_all_weights(perceptron_array_2d):\n",
        "#     for perceptron_array in perceptron_array_2d:\n",
        "#         list_of_weights = list(map(lambda x: x.weights, perceptron_array))\n",
        "#         print(list_of_weights)\n",
        "\n",
        "### Helper math functions\n",
        "def apply_sigmoid_function(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "# print_all_weights(character_classifier_model(1, 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "9GZXFHh5VMtT"
      },
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, weights=1, output=1, inputs =1):\n",
        "        self.weights = weights\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "    def think(self):\n",
        "        total = sum([weight*input for weight, input in zip(self.weights, self.inputs)])\n",
        "        self.output = apply_sigmoid_function(total)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Declare number of hidden layers\n",
        "# hidden_layers = 2\n",
        "# # Declare number of perceptions per layer\n",
        "# ### Play around with this number - does having less than output matter?\n",
        "\n",
        "\n",
        "def initialise_weights(num_weights):\n",
        "    output = []\n",
        "    for i in range(num_weights):\n",
        "        output.append(random.uniform(0,1))\n",
        "    return output\n",
        "        \n",
        "## TODO: Replace create_input_layer with image to vector implementation\n",
        "\n",
        "INPUT_LAYER_SIZE = 784\n",
        "def create_input_layer():\n",
        "  output = []\n",
        "  for i in range(INPUT_LAYER_SIZE):\n",
        "    output.append(Perceptron(output=random.uniform(0,1)))\n",
        "    return output\n",
        "  \n",
        "def create_output_layer(num_inputs, num_outputs=62):\n",
        "  output = []\n",
        "  for i in range(num_outputs):\n",
        "    output.append(Perceptron(weights=initialise_weights(num_inputs)))\n",
        "  return output\n",
        "\n",
        "### Helper initialisation functions\n",
        "def initialise_perceptron_layers(perceptrons_per_layer = 100):\n",
        "    model = []\n",
        "    input_layer = create_input_layer()\n",
        "    model.append(input_layer) # Creates a list of 784 elements of INPUT SIZE -> model = [[inputs(784 length)]]\n",
        "    perceptron_array = []\n",
        "    for _ in range(perceptrons_per_layer):\n",
        "        perceptron_array.append(Perceptron(weights=initialise_weights(len(input_layer))))\n",
        "    model.append(perceptron_array) # Now we have model = [[inputs:list], [perceptrons:list]]\n",
        "    model.append(create_output_layer(len(perceptron_array))) # Now we have a vector-like array = [[inputs:list], [perceptrons:list], [output_layer:list]]\n",
        "    return model\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<__main__.Perceptron object at 0x1130ba410>, <__main__.Perceptron object at 0x1130b8490>, <__main__.Perceptron object at 0x1130baf50>, <__main__.Perceptron object at 0x1130ba810>, <__main__.Perceptron object at 0x1130b8ed0>, <__main__.Perceptron object at 0x1130ba390>, <__main__.Perceptron object at 0x1130b8d50>, <__main__.Perceptron object at 0x1130bb650>, <__main__.Perceptron object at 0x1130bb010>, <__main__.Perceptron object at 0x1130b87d0>, <__main__.Perceptron object at 0x1130b8d90>, <__main__.Perceptron object at 0x1130bb710>, <__main__.Perceptron object at 0x1130b80d0>, <__main__.Perceptron object at 0x1130ba4d0>, <__main__.Perceptron object at 0x1130bbad0>, <__main__.Perceptron object at 0x1130b9f90>, <__main__.Perceptron object at 0x1130baa50>, <__main__.Perceptron object at 0x1130ba610>, <__main__.Perceptron object at 0x1130b8450>, <__main__.Perceptron object at 0x1130bbc90>, <__main__.Perceptron object at 0x1130b8f10>, <__main__.Perceptron object at 0x1130b9d90>, <__main__.Perceptron object at 0x1130bb3d0>, <__main__.Perceptron object at 0x1130b8710>, <__main__.Perceptron object at 0x1130baa90>, <__main__.Perceptron object at 0x1131858d0>, <__main__.Perceptron object at 0x113184910>, <__main__.Perceptron object at 0x113184990>, <__main__.Perceptron object at 0x113184f90>, <__main__.Perceptron object at 0x1131856d0>, <__main__.Perceptron object at 0x113184090>, <__main__.Perceptron object at 0x113184450>, <__main__.Perceptron object at 0x113184790>, <__main__.Perceptron object at 0x1131840d0>, <__main__.Perceptron object at 0x1131868d0>, <__main__.Perceptron object at 0x113184e90>, <__main__.Perceptron object at 0x113184350>, <__main__.Perceptron object at 0x113186bd0>, <__main__.Perceptron object at 0x113186d90>, <__main__.Perceptron object at 0x113186ad0>, <__main__.Perceptron object at 0x113187090>, <__main__.Perceptron object at 0x113242090>, <__main__.Perceptron object at 0x113241190>, <__main__.Perceptron object at 0x113242710>, <__main__.Perceptron object at 0x113241450>, <__main__.Perceptron object at 0x113241690>, <__main__.Perceptron object at 0x113242310>, <__main__.Perceptron object at 0x113242c50>, <__main__.Perceptron object at 0x113241010>, <__main__.Perceptron object at 0x113242bd0>, <__main__.Perceptron object at 0x11324ed90>, <__main__.Perceptron object at 0x11324f590>, <__main__.Perceptron object at 0x11324f110>, <__main__.Perceptron object at 0x11324fd50>, <__main__.Perceptron object at 0x11324c1d0>, <__main__.Perceptron object at 0x11324e4d0>, <__main__.Perceptron object at 0x11324cdd0>, <__main__.Perceptron object at 0x11324f890>, <__main__.Perceptron object at 0x11324d0d0>, <__main__.Perceptron object at 0x11324d190>, <__main__.Perceptron object at 0x11324fe50>, <__main__.Perceptron object at 0x11324e350>]\n",
            "[0.9999999999839742, 0.9999999999993534, 0.9999999999994655, 0.9999999999980411, 0.999999999998407, 0.9999999999999638, 0.9999999999993625, 0.9999999999998046, 0.9999999999993758, 0.9999999999967804, 0.9999999999948177, 0.99999999999979, 0.9999999999952025, 0.9999999999988833, 0.9999999999997105, 0.9999999999993088, 0.999999999999579, 0.9999999999989735, 0.9999999999998788, 0.9999999999991693, 0.9999999999986229, 0.999999999999881, 0.9999999999955456, 0.9999999999989269, 0.9999999999894069, 0.999999999996616, 0.9999999999999363, 0.999999999999281, 0.999999999998961, 0.9999999999851936, 0.9999999999996687, 0.9999999999998082, 0.9999999999948697, 0.9999999999996994, 0.9999999999923375, 0.9999999999971907, 0.9999999999985094, 0.9999999999984996, 0.9999999999973825, 0.9999999999919937, 0.9999999999973825, 0.9999999999998632, 0.9999999999966296, 0.9999999999992659, 0.9999999999939557, 0.9999999999995883, 0.9999999999996294, 0.9999999999974705, 0.9999999999885099, 0.9999999999950939, 0.9999999999999767, 0.9999999999990481, 0.9999999999991491, 0.999999999999807, 0.9999999999869196, 0.9999999999999369, 0.9999999999953761, 0.9999999999995641, 0.9999999999957512, 0.9999999999979172, 0.9999999999996805, 0.9999999999996994]\n",
            "62\n"
          ]
        }
      ],
      "source": [
        "# This is an array of arrays = [[inputs:list], [perceptrons:list], [output_layer:list]]\n",
        "my_neural_net = initialise_perceptron_layers()\n",
        "\n",
        "def model_think(model):\n",
        "  for i in range(len(model)):\n",
        "    if i == 0:\n",
        "      continue\n",
        "    for perceptron in model[i]:\n",
        "      perceptron.inputs = [perceptron.output for perceptron in model[i-1]]\n",
        "      perceptron.think()\n",
        "  print (model[-1])\n",
        "  print ([perceptron.output for perceptron in model[-1]])\n",
        "  print (len(model[-1]))\n",
        "\n",
        "model_think(my_neural_net)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pending Questions\n",
        "\n",
        "- Q: Does a perceptron have just 1 or many weights? Is it a weight per synapse/per input? How do we represent this in OOP?\n",
        "- Q: Does the sigmoid function get applied to each perception's weight? Or how does it get activated? What is the input (x) for the sigmoid function?\n",
        "  - A: Each neuron's output is processed by the sigmoid function first, so that every neuron's output becomes 0 < output` < 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
